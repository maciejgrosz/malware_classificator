import json
import os

# Load the extracted features from CAPA
with open("./tmp/tmp_extracted_capa_features.json", "r") as file:
    capa_data = json.load(file)

# Load the features from GHIDRA
with open("./tmp/tmp_ghidra_features.json", "r") as file:
    ghidra_data = json.load(file)

# Extract relevant data from CAPA
capa_meta = capa_data.get("meta", {})
capa_capabilities = capa_data.get("capabilities", [])

# Flatten CAPA capabilities into a dictionary with counts
capa_capability_counts = {}
for capability in capa_capabilities:
    capa_capability_counts[capability["capability"]] = capability["matches"]

# Extract relevant data from GHIDRA
ghidra_stats = {
    "data_size": ghidra_data.get("data_size"),
    "code_size": ghidra_data.get("code_size"),
    "code_data_ratio": ghidra_data.get("code_data_ratio"),
    "function_count": ghidra_data.get("function_count"),
    "export_count": ghidra_data.get("export_count"),
    "import_count": ghidra_data.get("import_count"),
}

# Add section details from GHIDRA data
ghidra_sections = {}
for section in ghidra_data.get("sections", []):
    section_name = section.get("name")
    ghidra_sections[section_name] = {
        "start": section.get("start"),
        "end": section.get("end"),
        "size": section.get("size"),
    }

# Merge data into a single dictionary
merged_data = {
    "path": capa_meta.get("path"),
    "md5": capa_meta.get("md5"),
    "sha1": capa_meta.get("sha1"),
    "sha256": capa_meta.get("sha256"),
    "os": capa_meta.get("os"),
}

# Add GHIDRA statistics and sections
merged_data.update(ghidra_stats)
merged_data["sections"] = ghidra_sections

# Add CAPA capabilities
merged_data.update(capa_capability_counts)

file_name = os.path.basename(capa_meta.get("path"))

# Save merged data to JSON file
output_file = f"../data/unformatted_data/analysis_{file_name}.json"
with open(output_file, "w") as outfile:
    json.dump(merged_data, outfile, indent=4)

print(f"Merged features saved to {output_file}")
